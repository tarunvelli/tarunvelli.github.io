<!DOCTYPE html>
<html lang="en-us">
  <head>
    <title>Self-Hosting Ollama | Tarun Vellishetty</title>

    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">    
<meta name="viewport" content="width=device-width,minimum-scale=1">
<meta name="description" content="How to install and run Ollama from a self-hosted server or VPS">
<meta name="generator" content="Hugo 0.108.0">


  <META NAME="ROBOTS" CONTENT="INDEX, FOLLOW">


<link rel="stylesheet" href="/css/style.css">



<link rel="shortcut icon" href="/images/favicon.ico" type="image/x-icon" />








  </head>

  <body>
    <nav class="navigation">
	
		<a href="/"> <span class="arrow">‚Üê</span>Home</a>
	

	
		<a href="/project">Projects</a>
	
		<a href="/post">Posts</a>
	
	<a href="/tags">Tags</a>

	
	  <a class="button" href="https://tarunvelli.site/index.xml">Subscribe</a>
	
</nav>


    <main class="main">
      

<section id="single">
    <h1 class="title">Self-Hosting Ollama</h1>

    <div class="tip">
        <time datetime="2024-06-16 00:00:00 &#43;0000 UTC">Jun 16, 2024</time>
        <span class="split">
          ¬∑
        </span>
        <span>
          653 words
        </span>
        <span class="split">
          ¬∑
        </span>
        <span>
          4 minute read
        </span>
    </div>

    
    


    <div class="content">
      <h1 id="what-ill-be-doing-in-this-guide">What I&rsquo;ll be doing in this guide <a href="#what-ill-be-doing-in-this-guide" class="anchor">üîó</a></h1><p>Ollama is an open-source app to run and customize large language models.</p>
<p>This is a step-by-step guide on how to install and run Ollama on a self-hosted server or VPS with Nginx. I&rsquo;ve used an Ubuntu droplet from DigitalOcean, but this can be adapted to pretty much any server environment. This setup is mainly for experimenting and might not be suitable for production applications.</p>
<p>Using GPU machines is preferred, and I tried setting up a GPU droplet using DigitalOcean&rsquo;s <a href="https://www.paperspace.com/" target="_blank" rel="noopener">Paperspace</a> product. I was running into issues with Paperspace to set up the GPU droplet. I needed to request approval and I was also running into an error when trying to access its billing integration page with DigitalOcean. So I decided to use a CPU machine while writing this guide as it was easier to set up and experiment with. Also, it didn&rsquo;t hurt that it&rsquo;s cheaper.</p>
<p><p class="markdown-image">
  <img src="/assets/ollama/paperspace-approval-required.png##center" alt="paperspace-approval-request"  />
</p></p>
<p>I&rsquo;ve set up the DigitalOcean droplet in the closest region to my users (just myself for this guide), using the Ubuntu 24.04 image, which is the latest LTS at the time of writing, with a dedicated general-purpose CPU and I&rsquo;ve selected an SSH key as the authentication method.</p>
<h1 id="why-would-you-want-to-try-this">Why would you want to try this <a href="#why-would-you-want-to-try-this" class="anchor">üîó</a></h1><p>I don&rsquo;t intend to convince anyone that self-hosting an LLM is better than using third-party services. This is just to enable developers so that they can experiment with self-hosting and evaluate it for themselves.</p>
<p>Some parameters to evaluate would be pricing, security, and the ability to customize the models. With self-hosting, we can have a flat rate per month instead of token usage-based pricing. The onus of securing the server lies with the developer. I&rsquo;ve picked Ollama as it allows <a href="https://github.com/ollama/ollama?tab=readme-ov-file#customize-a-model" target="_blank" rel="noopener">customizing models</a>.</p>
<h1 id="how-to-self-host-an-llm-using-ollama">How to self-host an LLM using Ollama <a href="#how-to-self-host-an-llm-using-ollama" class="anchor">üîó</a></h1><h2 id="install-and-set-up-ollama">Install and set up Ollama <a href="#install-and-set-up-ollama" class="anchor">üîó</a></h2><p>After creating the droplet, access it from your terminal by SSH:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>ssh root@&lt;ip address&gt;
</span></span></code></pre></div><p>I always install vim first as I prefer it over nano:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>sudo apt update
</span></span><span style="display:flex;"><span>sudo apt -y install vim
</span></span></code></pre></div><p>Install Ollama using the one-liner; this installs Ollama and starts a systemd service:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>curl -fsSL https://ollama.com/install.sh | sh
</span></span></code></pre></div><p><a href="https://github.com/ollama/ollama/blob/main/docs/linux.md" target="_blank" rel="noopener">Ollama has additional instructions</a> for manual installation or using it with GPUs.</p>
<p>Pull the required model. I&rsquo;ve used llama3 here:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>ollama pull llama3
</span></span></code></pre></div><p>Test the REST API by querying localhost:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>curl http://localhost:11434/api/generate -d &#39;{
</span></span><span style="display:flex;"><span>  &#34;model&#34;: &#34;llama3&#34;,
</span></span><span style="display:flex;"><span>  &#34;prompt&#34;:&#34;Why is the sky blue?&#34;
</span></span><span style="display:flex;"><span>}&#39;
</span></span></code></pre></div><h2 id="install-and-set-up-nginx">Install and set up Nginx <a href="#install-and-set-up-nginx" class="anchor">üîó</a></h2><p>To make the API accessible from external machines, we can use Nginx as a reverse proxy and also to set up basic auth.</p>
<p>Install Nginx and apache2-utils:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>sudo apt update
</span></span><span style="display:flex;"><span>sudo apt -y install nginx apache2-utils
</span></span></code></pre></div><p>Create a user (ollama_user), then specify and confirm a password:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>sudo htpasswd -c /etc/nginx/.htpasswd ollama_user
</span></span></code></pre></div><p>Edit the default Nginx config file:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>sudo vim /etc/nginx/sites-available/default
</span></span></code></pre></div><p>Enter the below configuration:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>server {
</span></span><span style="display:flex;"><span>    listen 80;
</span></span><span style="display:flex;"><span>    listen [::]:80;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    server_name _;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    location / {
</span></span><span style="display:flex;"><span>        proxy_set_header   X-Forwarded-For $remote_addr;
</span></span><span style="display:flex;"><span>        proxy_set_header   Host $http_host;
</span></span><span style="display:flex;"><span>        proxy_pass         &#34;http://127.0.0.1:11434&#34;;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        auth_basic &#34;Restricted Content&#34;;
</span></span><span style="display:flex;"><span>        auth_basic_user_file /etc/nginx/.htpasswd;
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>Remove the existing default file in sites-enabled (if present) and replace it with a soft link to sites-available:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>sudo rm /etc/nginx/sites-enabled/default
</span></span><span style="display:flex;"><span>sudo ln -s /etc/nginx/sites-available/default /etc/nginx/sites-enabled/default
</span></span></code></pre></div><p>Reload Nginx:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>sudo service nginx reload
</span></span></code></pre></div><p>Now the API is accessible from an external machine:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>curl -u ollama_user:&lt;password&gt; http://&lt;ip address&gt;/api/generate -d &#39;{
</span></span><span style="display:flex;"><span>  &#34;model&#34;: &#34;llama3&#34;,
</span></span><span style="display:flex;"><span>  &#34;prompt&#34;:&#34;Why is the sky blue?&#34;
</span></span><span style="display:flex;"><span>}&#39;
</span></span></code></pre></div><p>When generating a response with <code>&quot;stream&quot;: false</code>, large responses can time out Nginx. To resolve this, we can increase the timeout period in Nginx.</p>
<p>Open the config:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>sudo vim /etc/nginx/sites-available/default
</span></span></code></pre></div><p>and update it to below to increase the timeout to 500 seconds:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>server {
</span></span><span style="display:flex;"><span>    listen 80;
</span></span><span style="display:flex;"><span>    listen [::]:80;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    server_name _;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    location / {
</span></span><span style="display:flex;"><span>        proxy_set_header   X-Forwarded-For $remote_addr;
</span></span><span style="display:flex;"><span>        proxy_set_header   Host $http_host;
</span></span><span style="display:flex;"><span>        proxy_pass         &#34;http://127.0.0.1:11434&#34;;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        proxy_connect_timeout       500;
</span></span><span style="display:flex;"><span>        proxy_send_timeout          500;
</span></span><span style="display:flex;"><span>        proxy_read_timeout          500;
</span></span><span style="display:flex;"><span>        send_timeout                500;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        auth_basic &#34;Restricted Content&#34;;
</span></span><span style="display:flex;"><span>        auth_basic_user_file /etc/nginx/.htpasswd;
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>Additionally, you can also enable the UFW firewall:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>sudo ufw allow OpenSSH
</span></span><span style="display:flex;"><span>sudo ufw allow &#39;Nginx Full&#39;
</span></span><span style="display:flex;"><span>sudo ufw enable
</span></span></code></pre></div><p>That&rsquo;s the basic setup to start experimenting with a self-hosted LLM.</p>

    </div>

    
        <div class="tags">
            
                <a href="https://tarunvelli.site/tags/ollama">ollama</a>
            
                <a href="https://tarunvelli.site/tags/self-hosting">self-hosting</a>
            
                <a href="https://tarunvelli.site/tags/llm">llm</a>
            
                <a href="https://tarunvelli.site/tags/nginx">nginx</a>
            
        </div>
    
    
    

</section>


    </main>
    
    <footer id="footer">
    
        <div id="social">


    <a class="symbol" href="mailto:tarunvelli@gmail.com" rel="me" target="_blank">
        
        <svg xmlns="http://www.w3.org/2000/svg" width="28" height="28" viewBox="0 0 24 24" stroke-width="2" stroke="#bbbbbb" fill="none" stroke-linecap="round" stroke-linejoin="round">
   <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
   <path d="M3 7a2 2 0 0 1 2 -2h14a2 2 0 0 1 2 2v10a2 2 0 0 1 -2 2h-14a2 2 0 0 1 -2 -2v-10z"></path>
   <path d="M3 7l9 6l9 -6"></path>
</svg>
    </a>

    <a class="symbol" href="https://github.com/tarunvelli" rel="me" target="_blank">
        
        <svg xmlns="http://www.w3.org/2000/svg" width="28" height="28" viewBox="0 0 24 24" stroke-width="2" stroke="#bbbbbb" fill="none" stroke-linecap="round" stroke-linejoin="round">
   <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
   <path d="M9 19c-4.3 1.4 -4.3 -2.5 -6 -3m12 5v-3.5c0 -1 .1 -1.4 -.5 -2c2.8 -.3 5.5 -1.4 5.5 -6a4.6 4.6 0 0 0 -1.3 -3.2a4.2 4.2 0 0 0 -.1 -3.2s-1.1 -.3 -3.5 1.3a12.3 12.3 0 0 0 -6.2 0c-2.4 -1.6 -3.5 -1.3 -3.5 -1.3a4.2 4.2 0 0 0 -.1 3.2a4.6 4.6 0 0 0 -1.3 3.2c0 4.6 2.7 5.7 5.5 6c-.6 .6 -.6 1.2 -.5 2v3.5"></path>
</svg>
    </a>

    <a class="symbol" href="https://www.linkedin.com/in/tarun-velli" rel="me" target="_blank">
        
        <svg xmlns="http://www.w3.org/2000/svg" width="28" height="28" viewBox="0 0 24 24" stroke-width="2" stroke="#bbbbbb" fill="none" stroke-linecap="round" stroke-linejoin="round">
   <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
   <path d="M4 4m0 2a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v12a2 2 0 0 1 -2 2h-12a2 2 0 0 1 -2 -2z"></path>
   <path d="M8 11l0 5"></path>
   <path d="M8 8l0 .01"></path>
   <path d="M12 16l0 -5"></path>
   <path d="M16 16v-3a2 2 0 0 0 -4 0"></path>
</svg>
    </a>


</div>

    

    
</footer>



  </body>
</html>
